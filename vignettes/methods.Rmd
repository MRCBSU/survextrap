---
title: "Details of methods behind survextrap"
author: "Christopher Jackson <chris.jackson@mrc-bsu.cam.ac.uk>"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
    theme: simplex
    code_folding: show
vignette: >
  %\VignetteIndexEntry{Details of methods behind survextrap}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r,echo=FALSE,message=FALSE}
knitr::opts_chunk$set(eval=TRUE)
```

# Technical details of the model

See the [README](../index.html) for the design principles of the package, and the [examples vignette](examples.html) for examples of how to fit models.

It is work in progress, so it will be sketchy in some places.


## M-splines 

The axis of time is split into a set of regions defined by _knots_.   In the figure below, the knots are located at the integers from 0 to 10, as shown by the grid lines.

Each knot is associated with a _cubic polynomial function_ $b_k(t)$, known as a _basis_ function.  M-spline basis functions are defined to be positive.

Here there are 11 basis functions in the interior of the space. Each basis function has a peak around the knot, and diminishes to zero on either side.  Two additional basis functions are used with peaks at the boundaries of the space, so there are $K=13$ basis functions in total.    See [Ramsay](https://www.jstor.org/stable/2245395) for the exact definitions of $b_k(t)$. 

The hazard $h(t)$ at time $t$ is defined by a _weighted sum_ of the basis functions:

$$h(t) = \eta \sum_k p_k b_k(t)$$ 

with 

* weights $p_1,\ldots,p_K$ that sum to 1, $\sum_k p_k = 1$, known as the _basis coefficients_.
    
* a _scale_ parameter $\eta$ that describes the average level of the hazard. 

```{r,message=FALSE,warning=FALSE,class.source = 'fold-hide'}
library(survextrap)
library(ggplot2)
p_naive <- rep(1/13, 13)
p_const <- mspline_uniform_weights(iknots=1:9, bknots=c(0,10))
haz_unif <- plot_mspline(df=13, scale=10, coefs = p_const, plot=FALSE)$hazard 
plot_mspline(df=13, scale=10, coefs = p_naive, tmax=10) + 
    geom_line(aes(x=time, y=haz), data=haz_unif, color="red", lwd=1.5, inherit.aes = FALSE) + 
    annotate(geom="text", x=2, y=1.5, color="red", label="h(t): weights `p_const`") +
    annotate(geom="text", x=1.5, y=2.5, color="blue", label="h(t): weights `p_naive`") + 
    xlab("Time t") + ylab("Hazard rate") + ylim(0,3) 
```


## Using M-splines to represent particular hazard shapes

The weights $p_k$ define how the hazard changes through time.   If knots are equally spaced, then $p_k$ are nearly proportional to the hazard in the $k$th region, so that if all $p_k$ are equal, then the hazard is nearly constant (red line `p_equal` in the figure).   It is not exactly constant because of the influence of the regions at either end of the space, whose polynomial functions have a skewed shape.  

To produce a practically constant hazard, we can search numerically for the $p_k$ which minimise the variance of a selection of points $t$ that span the area that we want to model.  This search can be done automatically in the package (red line `p_const` in the figure). 

 **Extrapolation?** To fully specify a parametric time-to-event model, we have to define the hazard for times greater than the time $t_{K}$ of the highest knot, which is 10 in the example.

The model in `survextrap` simply assumes that the hazard for $t > t_{K}$ is constant, the same as the hazard at $t_{K}$. 

Therefore, if you want to estimate survival over a period where you think the hazard might not be constant, then you should make sure that the highest spline knot $t_K$ is after the end of this period.   

(_Sidenote_. Another way to define the hazard beyond the last knot would be to extrapolate the final basis polynomials, but this is not implemented. Although it would lead to a more smoothly extrapolated hazard, it would be driven by data just before the boundary, and would be making opaque assumptions about how the hazard changes after the boundary.  The idea behind the package is to make all assumptions explicit as data or mechanisms.  Perhaps a better approach would be to make the function smooth at the final knot but still constant after then, somehow). 

Examples of doing this with different kinds of data and knowledge are given in `vignette("examples")`.

Another arbitrary example of an M-spline curve:
```{r}
plot_mspline(iknots=c(1,3,5), bknots=c(0, 7),
     coefs=c(0.2, 0.3, 0.1, 0.1, 0.3, 0.1, 0.1), tmax=15)
```


#### Why not other kinds of spline?

* The Royston/Parmar model (e.g. in flexsurv) uses a natural cubic spline for the log cumulative hazard function.  A cumulative hazard is defined to be a non-decreasing function, but the spline is not constrained.   The constraint is handled during fitting the model - spline coefficients which imply non-increasing cumulative hazard functions are rejected.  This is inefficient.

* We could also have placed an unrestricted spline on the log hazard.   The disadvantage with this is that the cumulative hazard (required to obtain the likelihood for censored data) then has to be evaluated by numerical integration, which is slow.  

The advantage of putting an M-spline on the hazard is that it respects the constraint that a hazard has to be non-negative, while it can be integrated analytically to produce the cumulative hazard. 

A remaining disadvantage is that they still rely on a choice of knot locations.  We hope to show that this does not matter much.  Suggestions of alternative model families are welcome.  Gaussian processes, perhaps?  Though these are conceptually much harder.   Does the GLM framework of Kearns et al make it easier to compute likelihoods for arbitrarily flexible models (at the cost of approximating the hazard with a piecewise-constant time grid?)?

* I'd assume there is no difference between different kinds of arbitrarily-flexible model (fractional polynomials, particular kinds of spline) in terms of how well they tend to fit observed data, because they are all designed to do that job very well.  The difference I think will be in their computational efficiency, and how easily we can devise cleary-understandable priors on their parameters.  Priors will be influential in the places where there are no data.  For extrapolation, we don't want to rely on opaque nuances of the functional forms used in specific model families. 


## Bayesian model specification 

The _parameters_ of the model, $p_k$ and $\eta$, are estimated from data using Bayesian inference.

The _flexibility_ of the fitted model is determined by two things. 

(a) The number $K$ and choice of knots.  This determines the _maximum potential flexibility_ of the hazard function.

(b) The _prior distribution_ on the basis coefficients $p_k$.  This plays the role of _smoothing_ the fitted hazard function.

The default approach taken by `survextrap` is to choose a large number of knots $K$, intended to accommodate all reasonable situations, and then _estimate_ the extent of smoothing needed from the data.

* With _more data_, the fitted curve is as flexible as necessary to represent the data.

* With _less data_, the fit is influenced more by the prior.

This is a smoother, Bayesian analogue of the common approach to spline model selection based on choosing $K$ to give the model with the lowest AIC.  It also similar in principle to the "penalised likelihood" methods implemented in e.g. the `mgcv` and `rstpm2` packages.


## Prior distribution for the hazard curve

A log-normal prior is used for the scale $\eta$.   Currently this is set to the log crude event rate plus normal(0, 20) (inspired by [rstanarm](https://arxiv.org/pdf/2002.09633.pdf)) - but this will be customisable in a future version.

A multinomial logistic distribution defines the prior for the coefficients $p_k$ that define the mass of the hazard in each region: $log(p_k / p_1) = \beta_k$, with $\beta_1=0$ and $\beta_k \sim Logistic(\mu_k, \sigma)$ for $k = 2, \ldots, K$.  This prior is defined by:

(a) **Prior mean**: $\boldsymbol{\mu} = (\mu_2,\ldots,\mu_K)$.  By default, this is determined from the special set of $p_k$ that result in a _constant hazard_ $h(t)$, REF graph above.

(b) **Prior variability**: $\sigma$ controls the smoothness of the fitted hazard curve.

    * If $\sigma=0$, then we are certain that the hazard function is the one defined by $\boldsymbol{\mu}$.

    * Values of $\sigma$ around 1 favour wiggly curves, such that the fitted hazard curve is driven mainly by the the observed data. 

    * For _very_ high values of $\sigma$, the hazard is dominated by a random one of the $K$ basis functions, which will not typically be useful in practice.

(_Sidenote_. This is similar to the Dirichlet distribution for the $p_k$, but is slightly better behaved for estimation.  While the interpretation of $\sigma$ is not completely intuitive, we hope to show empirically that this prior works usefully well in practice) 

The plots below show samples from the prior distribution for the hazard functions implied by 
two choices of the joint distribution for $\eta$ and $p$.  In both, $\eta$ is log normal(0,1), which supports hazard rates between about 0.1 and 7, and the prior mean for $p$ is the special set that is centred around a constant hazard.  The two samples differ by the choice of prior variability.  $\sigma=0.05$ is a strong prior that forces the hazard to be nearly uniform, while $\sigma=1$ allows a wide range of wiggly shapes.   

```{r,fig.height=3,class.source = 'fold-hide'}
iknots <- c(1:9); bknots <- c(0,10)
p_mean <- mspline_uniform_weights(iknots, bknots)
prior_mean <- log(p_mean[-1] / p_mean[1])
#prior_mean <- rep(1, length(knots) + 4)
set.seed(1)
p1 <- plot_mspline_priorpred(iknots=iknots, bknots=bknots, tmax=max(bknots)+1, 
                       prior_mean=prior_mean, prior_sd=0.05, scale_sd=1, nsim=10) + 
    ylim(0, 1) + ggtitle(bquote(sigma==0.05))
p2 <- plot_mspline_priorpred(iknots=iknots, bknots=bknots, tmax=max(bknots)+1, 
                       prior_mean=prior_mean, prior_sd=1, scale_sd=1, nsim=10) + 
    ylim(0, 1) + ggtitle(bquote(sigma==1))
gridExtra::grid.arrange(p1, p2, nrow=1)
```

By default, the package estimates $\sigma$ as part of the full Bayesian model (with a gamma(2,1) prior), so that uncertainty about the level of smoothness is accounted for. 

If sampling is poor, then this alternative procedure might work - fit the model in two steps ("empirical Bayes"):

1. A "training" model fit is perfomed, where a prior is placed on $\sigma$, and the maximum a posteriori estimate $\hat\sigma$ of $\sigma$ is determined.

2. A "final" model is then fitted, in which $\sigma$ is fixed to $\hat\sigma$, giving the "optimal" amount of smoothness for the hazard $h(t)$.

Either way, these procedures still need to be tested empirically to see whether they give sensible results.   It seems to under-smooth in some cases. Though it may not matter if the hazard function is under-smoothed, if the purpose of the model is to estimate a quantity that is an average over time (e.g. mean survival).

A sensible route to resolving these uncertainties might be use cross-validation to assess and choose between model specifications.  The package provides an interface to the ([loo](https://mc-stan.org/users/interfaces/loo)) method for Bayesian cross-validation - though note that this only assesses fit to the individual-level data, using the same principle as AIC.


## Choice of knots when modelling data

By default, internal knots are placed at the quantiles of the vector comprising the uncensored survival times in the individual level data and the distinct follow-up times in the external data. (Smarter suggestions are welcome for this).

6 internal knots are used, which (with cubic polynomials) leads to $K=10$ basis functions [check this notation] and parameters 

Users can override this default however, and place knots at any position. 

By default, the lower boundary knot is set to 0.   The upper boundary knot is set to the highest follow-up time in the data.    This default should be overridden if you want to extrapolate, and you think the hazard might change.   A sensible guide is to place the upper knot at the latest time that you are interested in modelling.  Beyond this time, you either think the hazard will not change, or any changes in the hazard do not matter for the purpose of the model.

The priors should then be chosen to give a low probability that the hazard varies outside the range thought reasonable, e.g. in terms of orders of magnitude.   The package should provide a nice way to convert beliefs of this kind into priors.

Currently, the number of knots does not depend on the absolute amount of data - should it?   With small datasets, there will be lots of knots and very few data points between them - and we will be relying more strongly on the prior to do the smoothing.  We could assess whether this matters by using cross-validation. 

With external data, we want a choice of knots outside the data which we can defend as allowing a reasonable range of shapes for the hazard function, prior to observing the data.
