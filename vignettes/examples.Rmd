---
title: "Examples of using survextrap"
author: "Christopher Jackson <chris.jackson@mrc-bsu.cam.ac.uk>"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
    theme: simplex
    code_folding: show
vignette: >
  %\VignetteIndexEntry{Examples of using survextrap}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r,echo=FALSE,message=FALSE}
knitr::opts_chunk$set(eval=TRUE, message=FALSE, warning=FALSE)
```


```{r,echo=FALSE,message=FALSE}
rstan::rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

This vignette gives examples of how to run `survextrap` to fit survival models.

See the [README](../index.html) for the design principles of the package, and the [methods vignette](methods.html) for technical details of the methods.

It is work in progress, so it will be sketchy in some places.   The code is supposed to work, but do not be surprised if there are bugs - please report any on [github issues](https://github.com/chjackson/survextrap/issues).

# Examples

For these examples, we use a dataset of trial of chemotherapy for colon cancer, provided as `colon` in the `survival` package, with an outcome of recurrence-free survival (`etype==1`) and artificially censored at 3 years.
```{r}
library(survextrap)
library(dplyr)
colonc <- colon |>
    mutate(years = time/365.25) |>
    filter(etype == 1) |>
    mutate(status = ifelse(years > 3, 0, status),
           years = ifelse(years > 3, 3, years))
survminer::ggsurvplot(survfit(Surv(years, status) ~ 1, data=colonc), data=colonc)
```


## No external data

The following is the simplest, default model in the package fitted to the short-term trial data alone. No external data are supplied.  After three years (the maximum follow up time of the short term data) 
the model assumes the hazard stays constant.   

The plot shows the posterior median and 95\% credible intervals for the survival and hazard functions.  The spline adapts to give a practically perfect fit to the short-term data - note the Kaplan-Meier curve is obscured by the fitted posterior median.   Although the model assumes the extrapolated hazard is constant, there is still a wide uncertainty interval around this constant value.  This might be thought to be plausible.

```{r,results="hide"}
nd_mod <- survextrap(Surv(years, status) ~ 1, data=colonc, chains=1)
plot(nd_mod, tmax=5)
```

The spline knots are shown as blue lines.  We could allow for the possibility of hazard changes beyond three years by placing spline knots beyond this point. 

The `basehaz_ops` argument is used to control the spline.  The `bknots` component of this is the vector comprising the lower and upper boundary knots.  Here we simply move the upper boundary knot from three to four years.

```{r,results="hide"}
nd_mod2 <- survextrap(Surv(years, status) ~ 1, data=colonc, chains=1, 
                     basehaz_ops = list(bknots = c(0, 4)))
plot(nd_mod2, tmax=5)
```

This increases the amount of uncertainty about the extrapolated survival and hazard.  

There are other ways we could have done this, such as changing the position of the knot, or adding extra knots around or beyond the upper part of the data.   The extrapolations are likely to be sensitive to the choice. 

A sensible guide is to place the upper knot at the latest time that you are interested in modelling.  Beyond this time, you either think the hazard will not change, or any changes in the hazard do not matter for the purpose of the model.

The priors should then be chosen to give a low probability that the hazard varies outside the range thought reasonable, e.g. in terms of orders of magnitude.   The package should provide a nice way to convert beliefs of this kind into priors.

But note that it is only necessary to extrapolate in this way, using knots and default priors, if there is **no substantive information** about the long term hazard!

In many practical situations of extrapolation in time-to-event models, we do have information.   For human survival, we at least know that people do not tend to live longer than 100 years.  There are usually data from national agencies about mortality of general populations. 

The idea of the package is to make this external information as explicit as possible - ideally in the form of data. 


## With a smaller dataset

Works nice and quickly.   Hazard curve suggests some overfitting and an excessive number of knots, though it's unclear how much that matters.

```{r,results="hide"}
nd_mod3 <- survextrap(Surv(futime, fustat) ~ 1, data=ovarian, chains=1)
plot(nd_mod3)
```



## Mean and restricted mean survival 

The functions `mean_survextrap` and `rmst_survextrap` calculate the mean and restricted mean survival time (RMST) from a fitted model.  Uncertainty is expressed by using the MCMC sample of the model parameters, and calculating the mean (or RMST) for each draw, which produces a sample from the posterior distribution.   The posterior median and credible intervals are computed by summarising this sample.

For particular combinations of parameter values, the mean under the model `nd_mod` cannot be calculated (resulting in an error message about a divergent integral), suggesting there is a substantial posterior probability that the mean is infinite.  

Instead we compute the restricted mean survival time $RMST(t)$ over three alternative time limits $t$: 3, 10 and 1000 years.  The upper 95\% credible limit for $RMST(1000)$ is very large. 

```{r}
# mean_survextrap(nd_mod)
rmst_survextrap(nd_mod2, c(3,10,1000))
```

We do not really believe that the mean survival for this population can be this large. 

This is evidence that the fitted model is unrealistic, and we should have included external information to bound the estimates of the mean within plausible values. 

This can be done with external data. 





## Extrapolation using long term population data 

Suppose we judge that after 5 years, the survival of these patients will be the same as in the general population, and we have some data describing the annual survival rates of a population who are similar to this one, perhaps from matching to national statistics or registry data by age and sex.  We would construct it something like this (though fake data are shown here).

```{r,results="hide"}
extdat <- data.frame(start = c(5, 10, 15, 20), 
                     stop =  c(10, 15, 20, 25), 
                     n = c(100, 100, 100, 100), 
                     r = c(50, 40, 30, 20))
nde_mod <- survextrap(Surv(years, status) ~ 1, data=colonc, 
                      chains=1, external = extdat)
plot(nde_mod)
```

```{r}
mean_survextrap(nde_mod)
rmst_survextrap(nde_mod, c(3, 10, 1000))
```

Including the external data gives a more confident extrapolation.   The mean is finite.  RMST converges to the mean as it is supposed to. 

At present, not much thought has gone into choice of knot locations with external data.  Currently these knots are based on just concatenating the event times in the individual data with the follow-up times in the external data, and then taking quantiles.   Suggestions are welcome for a smarter approach.   Knots in the middle of follow-up intervals?



## Expert elicitation on the long term

Information about long term survival could be elicited from experts.  To use this information about the model, we should also elicit the expert's _uncertainty_.

For example, we ask the expert to consider a set of people who have survived for 10 years.  How many of them would they expect to survive a further 5 years?  Through some kind of formal elicitation process, they supply a best guess (median) of 30%, and a 95\% credible interval of 10\% to 50\%.  

Using standard techniques from elicitation ([SHELF](http://www.jeremy-oakley.staff.shef.ac.uk/shelf/)) we can interpret that as a $Beta(6.6, 15.0)$ prior distribution for the survival probability.  

```{r}
SHELF::fitdist(vals=c(0.1, 0.3, 0.5), probs=c(0.025, 0.5, 0.975),
               lower=0, upper=1)$Beta
```

We can interpret this as the posterior from having observed $y=6$ survivors out of $n=20$ people (recalling the posterior from a $Binomial(y, n)$ combined with a vague $Beta(0.5, 0.5)$ prior is $Beta(y+0.5, n-y+0.5)$, and rounding $n$ and $y$ to whole numbers).

So the expert's judgment is equivalent to the information in an external dataset of the form 

<table> 
<tr>
<th colspan="2">Follow-up period </th>
<th colspan="2">Number</th>
</tr> 
<tr><th>Start time $t$</th><th>End time $u$</th><th>Alive at $t$</th><th>Still alive at $u$</th></tr>

<tr>
<td> 10 </td>
<td> 15 </td>
<td> 20 </td>
<td> 6  </td>
</tr>
</table>

and we can use it in a `survextrap` model as follows: 
```{r,results="hide"}
extdat <- data.frame(start = c(10), stop =  c(15), 
                     n = c(20), r = c(6))
nde_mod <- survextrap(Surv(years, status) ~ 1, data=colonc,
                      chains=1, external = extdat)
plot(nde_mod)
```

```{r}
mean_survextrap(nde_mod)
```

There is still substantial uncertainty about the mean, even with this level of information, but comparing to the second model above (`nd_mod2`), it is better than no information at all.    More investigation of the role of the knot placement here (blue lines in the figure) might be wise - particularly the one at 15 years.

This approach might be extended to include elicited values from multiple time points, or considering multiple experts.   In each case the elicited information can be converted straightforwardly into an aggregate table for use in `survextrap`.   




## Covariates

`survextrap` uses a proportional hazards model to represent covariates.    In the example here, we model survival by treatment group `rx`, which is a factor with three levels.  First fit a standard Cox model: 

```{r}
coxph(Surv(years, status) ~ rx, data=colonc)
```

Then fit a `survextrap` model and extract the log hazard ratios.  These agree with the Cox model - as expected, as we are using a proportional hazards model with a very flexible baseline hazard function.
```{r}
rxph_mod <- survextrap(Surv(years, status) ~ rx, data=colonc, chains=1, refresh=0)
summary(rxph_mod) |>
    filter(variable=="loghr")
```

The posterior median survival curves (thick lines) agree with the subgroup-specific Kaplan-Meier estimates (thin lines).
```{r}
plot(rxph_mod, niter=100)
```

Any number of covariates, categorical or continuous, can be included. 

We can also have covariates in the external data.   If covariates are included in the model formula, and an external dataset is supplied, then we must specify covariate values for each row of the external dataset.   (If the covariates are factors, then they can be supplied as character vectors in `external`, but the values should be taken from the factor levels in the internal data [todo: this probably isn't a necessary restriction])

For example, the external data might be assumed to have the same survival as the control group of the trial (corresponding to a value of `"Obs"` for the variable `rx`).

```{r}
extdat <- data.frame(start = c(5, 10), stop =  c(10, 15), 
                     n = c(100, 100), r = c(50, 40), 
                     rx = "Obs")
rxphe_mod <- survextrap(Surv(years, status) ~ rx, data=colonc, 
                      chains=1, external = extdat, refresh=0)
rmst_survextrap(rxphe_mod, niter=100, t=20)
plot(rxphe_mod, niter=100, tmax=5)
plot_hazard(rxphe_mod, niter=100, tmax=20)
```




## Cure models

Generate a dataset with an obvious cure fraction

```{r,results="hide",cache=TRUE}
library(flexsurvcure)
set.seed(1)
t <- rmixsurv(qweibull, n=1000, theta=0.5, shape=1.5, scale=1.2)
censtime <- 10
cure_df <- data.frame(t = pmin(t, censtime), status = as.numeric(t < censtime)) 
plot(survfit(Surv(t, status) ~ 1, data=cure_df))

## These are slow to sample 
noncure_mod <- survextrap(Surv(t, status) ~ 1, data=cure_df, chains=1, 
                          cure=FALSE)
plot(noncure_mod) # doesn't quite capture the curve

## This is very slow, but it fits  
cure_mod <- survextrap(Surv(t, status) ~ 1, data=cure_df, chains=1, cure=TRUE, modelid=1)
plot(cure_mod, tmax=10)
```

Should we allow covariates on the cure fraction as well, as flexsurvcure does by default?



## Relative survival models

Not implemented yet.  Could easily support a constant background hazard in a conventional additive hazard model.  

Could also use the proportional hazards model, as above, supplying background survival as external count data and estimating the hazard ratio.
